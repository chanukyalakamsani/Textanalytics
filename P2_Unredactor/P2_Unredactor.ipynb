{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features of name from Training Text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import io\n",
    "import os\n",
    "import pdb\n",
    "import sys\n",
    "\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk import ne_chunk\n",
    "\n",
    "\n",
    "from nltk.chunk import tree2conlltags\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "abc = []\n",
    "persons = []\n",
    "nwordsp= []\n",
    "totchp = []\n",
    "firstword = []\n",
    "secondword = []\n",
    "thirdword = []\n",
    "fourthword = []\n",
    "def get_entity(text):\n",
    "    \"\"\"Prints the entity inside of the text.\"\"\"\n",
    "    \n",
    "    BC = u'\\u2588'  \n",
    "    for sent in sent_tokenize(text):\n",
    "        for chunk in ne_chunk(pos_tag(word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label') and chunk.label() == 'PERSON':\n",
    "                #print(chunk.label(), ' '.join(c[0] for c in chunk.leaves()))\n",
    "                #print (len(chunk))\n",
    "                nwordsp.append(len(chunk))\n",
    "                s=\"\"\n",
    "                k=0\n",
    "                for c in chunk.leaves():\n",
    "                    #print(c[0])\n",
    "                    s+=c[0]\n",
    "                    s+=\" \"\n",
    "                    #print(s)\n",
    "                    if len(chunk) == 1:\n",
    "                        firstword.append(len(c[0]))\n",
    "                        secondword.append(0)\n",
    "                        thirdword.append(0)\n",
    "                        fourthword.append(0)\n",
    "                    if len(chunk) == 2 and k==0:\n",
    "                        firstword.append(len(c[0]))\n",
    "                    if len(chunk) == 2 and k==1:\n",
    "                        secondword.append(len(c[0]))\n",
    "                        thirdword.append(0)\n",
    "                        fourthword.append(0)\n",
    "                    if len(chunk) == 3 and k==0:\n",
    "                        firstword.append(len(c[0]))\n",
    "                    if len(chunk) == 3 and k==1:\n",
    "                        secondword.append(len(c[0]))\n",
    "                    if len(chunk) == 3 and k==2:\n",
    "                        thirdword.append(len(c[0]))\n",
    "                        fourthword.append(0)\n",
    "                    if len(chunk) == 4 and k==0:\n",
    "                        firstword.append(len(c[0]))\n",
    "                    if len(chunk) == 4 and k==1:\n",
    "                        secondword.append(len(c[0]))\n",
    "                    if len(chunk) == 4 and k==2:\n",
    "                        thirdword.append(len(c[0]))\n",
    "                    if len(chunk) == 4 and k==3:\n",
    "                        fourthword.append(len(c[0]))\n",
    "                    k=k+1\n",
    "                    \n",
    "                    abc.append(c[0])\n",
    "                persons.append(s[:-1])\n",
    "                #print(len(s[:-1]))\n",
    "                totchp.append(len(s[:-1]))  \n",
    "#    print (persons)\n",
    "#    print(len(persons))\n",
    "#    for i in abc:\n",
    "#        if i in text:\n",
    "#            l=len(i)\n",
    "#            text = text.replace(i,BC*l) \n",
    "#    print (text)\n",
    "#    print (nwordsp)\n",
    "#    print (totchp)\n",
    "#    print (abc)\n",
    "#    print (firstword)\n",
    "#    print(len(firstword))\n",
    "#    print (secondword)\n",
    "#    print (thirdword)\n",
    "\n",
    "    \n",
    "def doextraction(glob_text):\n",
    "    \"\"\"Get all the files from the given glob and pass them to the extractor.\"\"\"\n",
    "    for thefile in glob.glob(glob_text):\n",
    "        with io.open(thefile, 'r', encoding='utf-8') as fyl:\n",
    "            text = fyl.read()\n",
    "            get_entity(text)\n",
    "\n",
    "\n",
    " \n",
    "#if __name__ == '__main__':\n",
    "#    # Usage: python3 entity-extractor.py 'train/pos/*.txt'\n",
    "doextraction('train/*.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the extracted features into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      count  totalch  first  sec  third  fourth\n",
      "0         1        8      8    0      0       0\n",
      "1         2       13      8    4      0       0\n",
      "2         2       13      8    4      0       0\n",
      "3         1        5      5    0      0       0\n",
      "4         1        8      8    0      0       0\n",
      "5         1        8      8    0      0       0\n",
      "6         1        8      8    0      0       0\n",
      "7         2       11      4    6      0       0\n",
      "8         1        5      5    0      0       0\n",
      "9         1        7      7    0      0       0\n",
      "10        2       14      5    8      0       0\n",
      "11        2       14      7    6      0       0\n",
      "12        2       15      5    9      0       0\n",
      "13        1        5      5    0      0       0\n",
      "14        1        7      7    0      0       0\n",
      "15        2       10      2    7      0       0\n",
      "16        1        6      6    0      0       0\n",
      "17        1        4      4    0      0       0\n",
      "18        1        6      6    0      0       0\n",
      "19        2       13      7    5      0       0\n",
      "20        2       16      6    9      0       0\n",
      "21        2       12      6    5      0       0\n",
      "22        1        4      4    0      0       0\n",
      "23        1        6      6    0      0       0\n",
      "24        1        4      4    0      0       0\n",
      "25        1        3      3    0      0       0\n",
      "26        1        5      5    0      0       0\n",
      "27        2       11      6    4      0       0\n",
      "28        1        4      4    0      0       0\n",
      "29        2       10      4    5      0       0\n",
      "...     ...      ...    ...  ...    ...     ...\n",
      "2910      2       13      4    8      0       0\n",
      "2911      2       14      7    6      0       0\n",
      "2912      2       11      5    5      0       0\n",
      "2913      2       13      4    8      0       0\n",
      "2914      2       10      5    4      0       0\n",
      "2915      2       17      7    9      0       0\n",
      "2916      2       11      5    5      0       0\n",
      "2917      1        5      5    0      0       0\n",
      "2918      1        9      9    0      0       0\n",
      "2919      2       13      6    6      0       0\n",
      "2920      2       14      6    7      0       0\n",
      "2921      1        8      8    0      0       0\n",
      "2922      1        6      6    0      0       0\n",
      "2923      1        6      6    0      0       0\n",
      "2924      2       11      5    5      0       0\n",
      "2925      2       11      4    6      0       0\n",
      "2926      2       11      4    6      0       0\n",
      "2927      2       11      4    6      0       0\n",
      "2928      2       11      4    6      0       0\n",
      "2929      2       11      4    6      0       0\n",
      "2930      1        6      6    0      0       0\n",
      "2931      2       11      4    6      0       0\n",
      "2932      1        9      9    0      0       0\n",
      "2933      1        4      4    0      0       0\n",
      "2934      1        6      6    0      0       0\n",
      "2935      2       11      6    4      0       0\n",
      "2936      2       10      4    5      0       0\n",
      "2937      3       14      6    2      4       0\n",
      "2938      2       10      3    6      0       0\n",
      "2939      2       12      5    6      0       0\n",
      "\n",
      "[2940 rows x 6 columns]\n",
      "                Persons\n",
      "0              Bromwell\n",
      "1         Bromwell High\n",
      "2         Bromwell High\n",
      "3                 Scott\n",
      "4              Bartlett\n",
      "5              Bartlett\n",
      "6              Bartlett\n",
      "7           Dave Bowman\n",
      "8                 Earth\n",
      "9               Mankind\n",
      "10       Scott Bartlett\n",
      "11       Michael Madsen\n",
      "12      Tamer Karadagli\n",
      "13                Story\n",
      "14              Michael\n",
      "15           Ed Furlong\n",
      "16               Madsen\n",
      "17                 Does\n",
      "18               Madsen\n",
      "19        Jordana Spiro\n",
      "20     Bourne Ultimatum\n",
      "21         Curtis Wayne\n",
      "22                 Karl\n",
      "23               Moncia\n",
      "24                 Lars\n",
      "25                  Von\n",
      "26                Trier\n",
      "27          Barton Fink\n",
      "28                 Lars\n",
      "29           Jane Fonda\n",
      "...                 ...\n",
      "2910      Tony Martinez\n",
      "2911     Richard Crenna\n",
      "2912        Kathy Nolan\n",
      "2913      Tony Martinez\n",
      "2914         Lydia Reed\n",
      "2915  Michael Winkelman\n",
      "2916        Kathy Nolan\n",
      "2917              Nolan\n",
      "2918          Broadside\n",
      "2919      Irving Pinkus\n",
      "2920     Kiddie Matin√©e\n",
      "2921           Cousteau\n",
      "2922             Loaded\n",
      "2923             Hayden\n",
      "2924        Corey Large\n",
      "2925        John Denver\n",
      "2926        John Denver\n",
      "2927        John Denver\n",
      "2928        John Denver\n",
      "2929        John Denver\n",
      "2930             Denver\n",
      "2931        John Denver\n",
      "2932          Christmas\n",
      "2933               John\n",
      "2934             Denver\n",
      "2935        Martin Ritt\n",
      "2936         Jane Fonda\n",
      "2937     Robert De Niro\n",
      "2938         Pat Barker\n",
      "2939       Union Street\n",
      "\n",
      "[2940 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "ytrain=pd.DataFrame(persons, columns=['Persons'])\n",
    "xtrain = pd.DataFrame(nwordsp, columns=['count'])\n",
    "xtrain['totalch']= totchp\n",
    "xtrain['first']= firstword\n",
    "xtrain['sec']= secondword\n",
    "xtrain['third']= thirdword\n",
    "xtrain['fourth']= fourthword\n",
    "\n",
    "print(xtrain)\n",
    "print (ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Machine learning metho Naive Bayes to train the model on train dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "Xtrain=xtrain.iloc[:,0:5]\n",
    "Ytrain=ytrain.iloc[:,0]\n",
    "model1 = clf.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using  K nearest neighbors to train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "model2 = knn.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redacting Files and saving them to Test 2 folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import io\n",
    "import os\n",
    "import pdb\n",
    "import sys\n",
    "\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk import ne_chunk\n",
    "\n",
    "\n",
    "from nltk.chunk import tree2conlltags\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "abc = []\n",
    "persons = []\n",
    "nwordsp= []\n",
    "totchp = []\n",
    "firstword = []\n",
    "secondword = []\n",
    "thirdword = []\n",
    "fourthword = []\n",
    "count = []\n",
    "firstw = []\n",
    "secondw = []\n",
    "thirdw = []\n",
    "fourthw = []\n",
    "totch = []\n",
    "fileno = []\n",
    "def get_entity(text):\n",
    "    \"\"\"Prints the entity inside of the text.\"\"\"\n",
    "    \n",
    "    #BC = u'\\u2588'\n",
    "    BC = 'Q'\n",
    "    for sent in sent_tokenize(text):\n",
    "        for chunk in ne_chunk(pos_tag(word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label') and chunk.label() == 'PERSON':\n",
    "                #print(chunk.label(), ' '.join(c[0] for c in chunk.leaves()))\n",
    "                #print (len(chunk))\n",
    "                nwordsp.append(len(chunk))\n",
    "                k=0\n",
    "                for c in chunk.leaves():\n",
    "                    abc.append(c[0])\n",
    "                #persons.append(s[:-1])\n",
    "                #print(len(s[:-1]))\n",
    "                #totchp.append(len(s[:-1]))  \n",
    "    \n",
    "    for i in abc:\n",
    "        if i in text:\n",
    "            l=len(i)\n",
    "            text = text.replace(i,BC*l) \n",
    "#    print (text)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    fileno.append(1)    \n",
    "    filename = \"test2/file\"+ str(len(fileno))+\".txt\"\n",
    "    file = open(filename, \"w\",encoding=\"utf-8\")\n",
    "    file.write(text) \n",
    "    file.close()\n",
    "\n",
    "    #    print (nwordsp)\n",
    "#    print (totchp)\n",
    "#    print (abc)\n",
    "#    print (firstword)\n",
    "#    print(len(firstword))\n",
    "#    print (secondword)\n",
    "#    print (thirdword)\n",
    "\n",
    "    \n",
    "def doextraction(glob_text):\n",
    "    \"\"\"Get all the files from the given glob and pass them to the extractor.\"\"\"\n",
    "    \n",
    "    for thefile in glob.glob(glob_text):\n",
    "        with io.open(thefile, 'r', encoding='utf-8') as fyl:\n",
    "            text = fyl.read()\n",
    "            get_entity(text)\n",
    "            \n",
    "\n",
    "\n",
    " \n",
    "#if __name__ == '__main__':\n",
    "#    # Usage: python3 entity-extractor.py 'train/pos/*.txt'\n",
    "doextraction('test/*.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting features of redacted names from the redacted files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 2, 2, 2]\n",
      "[6, 7, 4, 5, 3]\n",
      "[7, 0, 7, 7, 7]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[14, 7, 12, 13, 11]\n",
      "[2, 1, 2, 2, 2]\n",
      "[6, 7, 4, 5, 3]\n",
      "[7, 0, 7, 7, 7]\n",
      "[0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0]\n",
      "[14, 7, 12, 13, 11]\n",
      "[2, 1, 2, 2, 2, 2, 2, 2, 2, 1]\n",
      "[6, 7, 4, 5, 3, 5, 6, 1, 1, 1]\n",
      "[7, 0, 7, 7, 7, 7, 0, 5, 4, 7]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[14, 7, 12, 13, 11, 13, 6, 7, 6, 9]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import io\n",
    "import os\n",
    "import pdb\n",
    "import sys\n",
    "\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk import ne_chunk\n",
    "\n",
    "\n",
    "from nltk.chunk import tree2conlltags\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "abc = []\n",
    "persons = []\n",
    "nwordsp= []\n",
    "totchp = []\n",
    "firstword = []\n",
    "secondword = []\n",
    "thirdword = []\n",
    "fourthword = []\n",
    "count = []\n",
    "firstw = []\n",
    "secondw = []\n",
    "thirdw = []\n",
    "fourthw = []\n",
    "totch = []\n",
    "fileno = []\n",
    "def get_entity(text):\n",
    "    \"\"\"Prints the entity inside of the text.\"\"\"\n",
    "    \n",
    "    #BC = u'\\u2588'\n",
    "    BC = 'Q'\n",
    "    for sent in sent_tokenize(text):\n",
    "        for chunk in ne_chunk(pos_tag(word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label') and chunk.label() == 'PERSON':\n",
    "                #print(chunk.label(), ' '.join(c[0] for c in chunk.leaves()))\n",
    "                #print (len(chunk))\n",
    "                nwordsp.append(len(chunk))\n",
    "                k=0\n",
    "                for c in chunk.leaves():\n",
    "                    abc.append(c[0])\n",
    "                #persons.append(s[:-1])\n",
    "                #print(len(s[:-1]))\n",
    "                #totchp.append(len(s[:-1]))  \n",
    "    \n",
    "    for i in abc:\n",
    "        if i in text:\n",
    "            l=len(i)\n",
    "            text = text.replace(i,BC*l) \n",
    "#    print (text)\n",
    "    \n",
    "#    for i in nltk.word_tokenize(text):\n",
    "#        m2 = re.search('^[\\w]$', i)\n",
    "#        if m2:\n",
    "#            print(\"m\")            \n",
    "#        else:\n",
    "#            print (i)\n",
    "    \n",
    "    array = []\n",
    "    #count = []\n",
    "    count1 = 0\n",
    "    wc = 0\n",
    "    wci = 0\n",
    "    first = 0\n",
    "    sec = 0\n",
    "    third = 0\n",
    "    fourth = 0\n",
    "    count2 = 0\n",
    "    for i in nltk.word_tokenize(text):\n",
    "        \n",
    "        m2 = re.search('^Q*Q$', i)\n",
    "        if m2:\n",
    "#            print(i)\n",
    "            wci = wci +1\n",
    "        else:\n",
    "            wc = wc + 1\n",
    "        if wci > wc:\n",
    "            count1 = count1 + 1\n",
    "        #    count2 = count2 + 1\n",
    "        if count1 != 0 and wc > wci:\n",
    "            count.append(count1)\n",
    "            count1 = 0\n",
    "        #if count1 = 1:\n",
    "        #    first = len(i)\n",
    "        wci = wc\n",
    "    #if count2 == 0:\n",
    "    #    ab = 0\n",
    "    #    count.append(ab)\n",
    "    print (count)\n",
    "    count3 = 0\n",
    "    wc = 0\n",
    "    wci = 0\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "#    print(tokens)\n",
    "    j = 0\n",
    "    for i in range(len(tokens)):\n",
    "        m2 = re.search('^Q*Q$', tokens[i])\n",
    "        if m2:\n",
    "#            print(tokens[i])\n",
    "            wci = wci +1\n",
    "        else:\n",
    "            wc = wc + 1\n",
    "        if wci > wc:\n",
    "            count3 = count3 + 1\n",
    "#            print (count3)\n",
    "        if count3 == 1:\n",
    "            first = len(tokens[i])\n",
    "            tot = first\n",
    "#            print('first')\n",
    "#            print(first)\n",
    "        if count3 == 2:\n",
    "            sec = len(tokens[i])\n",
    "            tot = first + sec +1\n",
    "#            print('second')\n",
    "#            print(sec)\n",
    "        if count3 == 3:\n",
    "            third = len(tokens[i])\n",
    "            tot = first + sec + third + 2\n",
    "#            print(third)\n",
    "        if count3 == 4:\n",
    "            fourth = len(tokens[i])\n",
    "            tot = first + sec + third + fourth + 3\n",
    "#            print(fourth)\n",
    "        if count3 == count[j]:\n",
    "            count3 = 0\n",
    "            firstw.append(first)\n",
    "            secondw.append(sec)\n",
    "            thirdw.append(third)\n",
    "            fourthw.append(fourth)\n",
    "            totch.append(tot)\n",
    "            first = 0\n",
    "            sec = 0\n",
    "            third = 0\n",
    "            fourth = 0\n",
    "            \n",
    "            \n",
    "            if j < len(count)-1:\n",
    "                j = j+1\n",
    "        wci = wc\n",
    "        tot =0\n",
    "        \n",
    "    #print(\"wc\")\n",
    "    print (firstw)\n",
    "    print (secondw)\n",
    "    print(thirdw)\n",
    "    print(fourthw)\n",
    "    print(totch)\n",
    "#    for i in range (len(count)):\n",
    "#        if count[i] == 1:\n",
    "#            tot = firstw[i]\n",
    "#        if count[i] == 2:\n",
    "#            tot = firstw[i] + secondw[i] + 1\n",
    "#        if count[i] == 3:\n",
    "#            tot = firstw[i] + secondw[i] + thirdw[i] + 2\n",
    "#       if count[i] == 4:\n",
    "#            tot = firstw[i] + secondw[i] + thirdw[i] + fourth[i] + 3\n",
    "#        totch.append(tot)\n",
    "    \n",
    "\n",
    "    #    print (nwordsp)\n",
    "#    print (totchp)\n",
    "#    print (abc)\n",
    "#    print (firstword)\n",
    "#    print(len(firstword))\n",
    "#    print (secondword)\n",
    "#    print (thirdword)\n",
    "\n",
    "    \n",
    "def doextraction(glob_text):\n",
    "    \"\"\"Get all the files from the given glob and pass them to the extractor.\"\"\"\n",
    "    \n",
    "    for thefile in glob.glob(glob_text):\n",
    "        with io.open(thefile, 'r', encoding='utf-8') as fyl:\n",
    "            text = fyl.read()\n",
    "            get_entity(text)\n",
    "            \n",
    "\n",
    "\n",
    " \n",
    "#if __name__ == '__main__':\n",
    "#    # Usage: python3 entity-extractor.py 'train/pos/*.txt'\n",
    "doextraction('test2/*.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the extracted test features into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   count  totalch  first  sec  third  fourth\n",
      "0      2       14      6    7      0       0\n",
      "1      1        7      7    0      0       0\n",
      "2      2       12      4    7      0       0\n",
      "3      2       13      5    7      0       0\n",
      "4      2       11      3    7      0       0\n",
      "5      2       13      5    7      0       0\n",
      "6      2        6      6    0      0       0\n",
      "7      2        7      1    5      0       0\n",
      "8      2        6      1    4      0       0\n",
      "9      1        9      1    7      0       0\n"
     ]
    }
   ],
   "source": [
    "#ytest=pd.DataFrame(persons, columns=['Persons'])\n",
    "xtest = pd.DataFrame(count, columns=['count'])\n",
    "xtest['totalch']= totch\n",
    "xtest['first']= firstw\n",
    "xtest['sec']= secondw\n",
    "xtest['third']= thirdw\n",
    "xtest['fourth']= fourthw\n",
    "\n",
    "print(xtest)\n",
    "#print (ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Naive Bayes and predicting the redacted names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Walter Matthau' 'Matthau' 'John Gilbert' 'Oprah Winfrey' 'Don Gallico'\n",
      " 'Oprah Winfrey' 'Davies' 'Al Lewis' 'De Niro' 'Al Izuruha']\n"
     ]
    }
   ],
   "source": [
    "Xtest=xtest.iloc[:,0:5]\n",
    "Ytest=ytest.iloc[:,0]\n",
    "\n",
    "\n",
    "result = model1.predict(Xtest)\n",
    "print(result)\n",
    "#from sklearn.metrics import accuracy_score\n",
    "#accuracy_score(result, Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Kmeans and predicting the redacted names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Walter Matthau' 'Matthau' 'Bebe Daniels' 'Celie Johnson' 'Ben Gazzara'\n",
      " 'Celie Johnson' 'Alicia' 'Al Lewis' 'De Niro' 'Al Izuruha']\n"
     ]
    }
   ],
   "source": [
    "ypred = model2.predict(Xtest)\n",
    "#print (knn.kneighbors(Xtest)[1])\n",
    "print(ypred)\n",
    "#from sklearn.metrics import accuracy_score\n",
    "#accuracy_score(ypred, Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting top 5 names using kmeans clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Walter Matthau', 'Walter Matthau', 'Lottie Wilkins', 'Claire Windsor', 'Stormy Weather'], ['Matthau', 'Whoopie', 'Matthau', 'Stanley', 'Stanley'], ['Jack Klugman', 'Bebe Daniels', 'Bebe Daniels', 'Jean Lafitte', 'Tony Randall'], ['Celie Johnson', 'James Stewart', 'Sybil Danning', 'Norma Shearer', 'Oprah Winfrey'], ['Mel Winkler', 'Red Skelton', 'Leo Kessler', 'Joe MacLeod', 'Ben Gazzara'], ['Celie Johnson', 'James Stewart', 'Sybil Danning', 'Norma Shearer', 'Oprah Winfrey'], ['Highly', 'Alicia', 'Alicia', 'Alisha', 'Marion'], ['De Niro', 'Al Lewis', 'Al Lewis', 'De Niro', 'Al Lewis'], ['De Niro', 'De Niro', 'Al Lewis', 'Got Fun', 'Mr. Bip'], ['Al Izuruha', 'Al Pacino', 'La Rocque', 'Ed Furlong', 'Al Izuruha']]\n"
     ]
    }
   ],
   "source": [
    "nearest5 = []\n",
    "list\n",
    "for i in range (len(knn.kneighbors(Xtest)[1])):\n",
    "    top5 = []\n",
    "    for j in range (len(knn.kneighbors(Xtest)[1][i])):\n",
    "        m= knn.kneighbors(Xtest)[1][i][j]\n",
    "        top5.append(Ytrain[m])\n",
    "    nearest5.append(top5)\n",
    "print(nearest5)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating euclidean and cosine distances between names and getting top 5 names closest to the test name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[273, 317, 371, 412, 444], [9, 14, 41, 42, 50], [495, 540, 568, 636, 694], [60, 64, 121, 316, 323], [105, 301, 585, 640, 1026], [60, 64, 121, 316, 323], [16, 18, 23, 36, 39], [295, 752, 797, 844, 880], [295, 2832, 572, 692, 752], [15, 429, 441, 609, 768]]\n",
      "[['Sabine Timento', 'Jiminy Cricket', 'Jiminy Cricket', 'Bernie Wiseman', 'Bernie Wiseman'], ['Mankind', 'Michael', 'Bergman', 'Utopian', 'Leopold'], ['John Gilbert', 'John Gilbert', 'John Gilbert', 'John Gilbert', 'Gina Yashere'], ['Uncle Kessler', 'Uncle Kessler', 'Uncle Kessler', 'Cliff Edwards', 'Norma Shearer'], ['Leo Kessler', 'Tom Kiesche', 'Joe College', 'Rod LaRoque', 'Gus Edwards'], ['Uncle Kessler', 'Uncle Kessler', 'Uncle Kessler', 'Cliff Edwards', 'Norma Shearer'], ['Madsen', 'Madsen', 'Moncia', 'Lesson', 'Dancer'], ['De Niro', 'Al Lewis', 'Al Lewis', 'Al Lewis', 'Al Lewis'], ['De Niro', 'De Niro', 'Got Fun', 'Mr. Bip', 'Al Lewis'], ['Ed Furlong', 'Al Izuruha', 'Al Izuruha', 'La Rocque', 'Al Pacino']]\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "distall = []\n",
    "for i in range (len(Xtest)):\n",
    "    distm = []\n",
    "    cos = []\n",
    "    ecd = []\n",
    "    for j in range (len(Xtrain)):\n",
    "        a1 = Xtrain.iloc[j,:]\n",
    "        a2 = Xtest.iloc[i,:]\n",
    "        ecd = distance.euclidean(a1,a2)\n",
    "        #cos1 = distance.cosine(a1,a2)\n",
    "        fdist = ecd\n",
    "        distm.append(fdist)\n",
    "    distall.append(distm)\n",
    "#print (distall)\n",
    "final5all=[]\n",
    "for i in range (len(distall)):\n",
    "    final5 = sorted(range(len(distall[i])), key=lambda x: distall[i][x])[:5]\n",
    "    final5all.append(final5)\n",
    "    \n",
    "print (final5all)\n",
    "\n",
    "nearest52 = []\n",
    "\n",
    "for i in range (len(final5all)):\n",
    "    top5 = []\n",
    "    for j in range (len(final5all[i])):\n",
    "        m= final5all[i][j]\n",
    "        top5.append(Ytrain[m])\n",
    "    nearest52.append(top5)\n",
    "print(nearest52)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing euclidean and cosine distance and adding them together to get a distance measure and getting the closest top 5 names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[273, 317, 371, 412, 444], [9, 14, 41, 42, 50], [495, 540, 568, 636, 694], [60, 64, 121, 316, 323], [105, 301, 585, 640, 1026], [60, 64, 121, 316, 323], [16, 18, 23, 36, 39], [752, 797, 844, 880, 960], [295, 2832, 752, 797, 844], [15, 429, 441, 609, 768]]\n",
      "[['Sabine Timento', 'Jiminy Cricket', 'Jiminy Cricket', 'Bernie Wiseman', 'Bernie Wiseman'], ['Mankind', 'Michael', 'Bergman', 'Utopian', 'Leopold'], ['John Gilbert', 'John Gilbert', 'John Gilbert', 'John Gilbert', 'Gina Yashere'], ['Uncle Kessler', 'Uncle Kessler', 'Uncle Kessler', 'Cliff Edwards', 'Norma Shearer'], ['Leo Kessler', 'Tom Kiesche', 'Joe College', 'Rod LaRoque', 'Gus Edwards'], ['Uncle Kessler', 'Uncle Kessler', 'Uncle Kessler', 'Cliff Edwards', 'Norma Shearer'], ['Madsen', 'Madsen', 'Moncia', 'Lesson', 'Dancer'], ['Al Lewis', 'Al Lewis', 'Al Lewis', 'Al Lewis', 'Al Lewis'], ['De Niro', 'De Niro', 'Al Lewis', 'Al Lewis', 'Al Lewis'], ['Ed Furlong', 'Al Izuruha', 'Al Izuruha', 'La Rocque', 'Al Pacino']]\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "\n",
    "distall = []\n",
    "for i in range (len(Xtest)):\n",
    "    distm = []\n",
    "    cos = []\n",
    "    ecd = []\n",
    "    for j in range (len(Xtrain)):\n",
    "        a1 = Xtrain.iloc[j,:]\n",
    "        a2 = Xtest.iloc[i,:]\n",
    "        ecd1 = distance.euclidean(a1,a2)\n",
    "        cos1 = distance.cosine(a1,a2)\n",
    "        ecd.append(ecd1)\n",
    "        cos.append(cos1)\n",
    "    norm1 = cos / np.linalg.norm(cos)\n",
    "    norm2 = ecd / np.linalg.norm(ecd)\n",
    "    distm = np.add(norm1,norm2)\n",
    "    distall.append(distm)\n",
    "#print (distall)\n",
    "final5all=[]\n",
    "for i in range (len(distall)):\n",
    "    final5 = sorted(range(len(distall[i])), key=lambda x: distall[i][x])[:5]\n",
    "    final5all.append(final5)\n",
    "    \n",
    "print (final5all)\n",
    "\n",
    "nearest52 = []\n",
    "\n",
    "for i in range (len(final5all)):\n",
    "    top5 = []\n",
    "    for j in range (len(final5all[i])):\n",
    "        m= final5all[i][j]\n",
    "        top5.append(Ytrain[m])\n",
    "    nearest52.append(top5)\n",
    "print(nearest52)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting name features from Test dataset without redaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import io\n",
    "import os\n",
    "import pdb\n",
    "import sys\n",
    "\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk import ne_chunk\n",
    "\n",
    "\n",
    "from nltk.chunk import tree2conlltags\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "abc = []\n",
    "persons = []\n",
    "nwordsp= []\n",
    "totchp = []\n",
    "firstword = []\n",
    "secondword = []\n",
    "thirdword = []\n",
    "fourthword = []\n",
    "def get_entity(text):\n",
    "    \"\"\"Prints the entity inside of the text.\"\"\"\n",
    "    \n",
    "    BC = u'\\u2588'  \n",
    "    for sent in sent_tokenize(text):\n",
    "        for chunk in ne_chunk(pos_tag(word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label') and chunk.label() == 'PERSON':\n",
    "                #print(chunk.label(), ' '.join(c[0] for c in chunk.leaves()))\n",
    "                #print (len(chunk))\n",
    "                nwordsp.append(len(chunk))\n",
    "                s=\"\"\n",
    "                k=0\n",
    "                for c in chunk.leaves():\n",
    "                    #print(c[0])\n",
    "                    s+=c[0]\n",
    "                    s+=\" \"\n",
    "                    #print(s)\n",
    "                    if len(chunk) == 1:\n",
    "                        firstword.append(len(c[0]))\n",
    "                        secondword.append(0)\n",
    "                        thirdword.append(0)\n",
    "                        fourthword.append(0)\n",
    "                    if len(chunk) == 2 and k==0:\n",
    "                        firstword.append(len(c[0]))\n",
    "                    if len(chunk) == 2 and k==1:\n",
    "                        secondword.append(len(c[0]))\n",
    "                        thirdword.append(0)\n",
    "                        fourthword.append(0)\n",
    "                    if len(chunk) == 3 and k==0:\n",
    "                        firstword.append(len(c[0]))\n",
    "                    if len(chunk) == 3 and k==1:\n",
    "                        secondword.append(len(c[0]))\n",
    "                    if len(chunk) == 3 and k==2:\n",
    "                        thirdword.append(len(c[0]))\n",
    "                        fourthword.append(0)\n",
    "                    if len(chunk) == 4 and k==0:\n",
    "                        firstword.append(len(c[0]))\n",
    "                    if len(chunk) == 4 and k==1:\n",
    "                        secondword.append(len(c[0]))\n",
    "                    if len(chunk) == 4 and k==2:\n",
    "                        thirdword.append(len(c[0]))\n",
    "                    if len(chunk) == 4 and k==3:\n",
    "                        fourthword.append(len(c[0]))\n",
    "                    k=k+1\n",
    "                    \n",
    "                    abc.append(c[0])\n",
    "                persons.append(s[:-1])\n",
    "                #print(len(s[:-1]))\n",
    "                totchp.append(len(s[:-1]))  \n",
    "#    print (persons)\n",
    "#    print(len(persons))\n",
    "#    for i in abc:\n",
    "#        if i in text:\n",
    "#            l=len(i)\n",
    "#            text = text.replace(i,BC*l) \n",
    "#    print (text)\n",
    "#    print (nwordsp)\n",
    "#    print (totchp)\n",
    "#    print (abc)\n",
    "#    print (firstword)\n",
    "#    print(len(firstword))\n",
    "#    print (secondword)\n",
    "#    print (thirdword)\n",
    "\n",
    "    \n",
    "def doextraction(glob_text):\n",
    "    \"\"\"Get all the files from the given glob and pass them to the extractor.\"\"\"\n",
    "    for thefile in glob.glob(glob_text):\n",
    "        with io.open(thefile, 'r', encoding='utf-8') as fyl:\n",
    "            text = fyl.read()\n",
    "            get_entity(text)\n",
    "\n",
    "\n",
    " \n",
    "#if __name__ == '__main__':\n",
    "#    # Usage: python3 entity-extractor.py 'train/pos/*.txt'\n",
    "doextraction('test/*.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the extracted dataset into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    count  totalch  first  sec  third  fourth\n",
      "0       1        6      6    0      0       0\n",
      "1       1        7      7    0      0       0\n",
      "2       2       12      4    7      0       0\n",
      "3       2       13      5    7      0       0\n",
      "4       2       11      3    7      0       0\n",
      "5       1        5      5    0      0       0\n",
      "6       1        7      7    0      0       0\n",
      "7       2       14      6    7      0       0\n",
      "8       1        5      5    0      0       0\n",
      "9       1        7      7    0      0       0\n",
      "10      2        9      4    4      0       0\n",
      "11      1        7      7    0      0       0\n",
      "           Persons\n",
      "0           Ashton\n",
      "1          Kutcher\n",
      "2     Jake Fischer\n",
      "3    Kevin Costner\n",
      "4      Ben Randall\n",
      "5            Kevin\n",
      "6          Costner\n",
      "7   Ashton Kutcher\n",
      "8            Kevin\n",
      "9          Costner\n",
      "10       Sela Ward\n",
      "11         Costner\n"
     ]
    }
   ],
   "source": [
    "ytest=pd.DataFrame(persons, columns=['Persons'])\n",
    "xtest = pd.DataFrame(nwordsp, columns=['count'])\n",
    "xtest['totalch']= totchp\n",
    "xtest['first']= firstword\n",
    "xtest['sec']= secondword\n",
    "xtest['third']= thirdword\n",
    "xtest['fourth']= fourthword\n",
    "\n",
    "print(xtest)\n",
    "print (ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying the Naive Bayes model on the test dataset and predicting names for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Davies' 'Matthau' 'John Gilbert' 'Oprah Winfrey' 'Don Gallico' 'Burns'\n",
      " 'Matthau' 'Walter Matthau' 'Burns' 'Matthau' 'Tsui Hark' 'Matthau']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest=xtest.iloc[:,0:5]\n",
    "Ytest=ytest.iloc[:,0]\n",
    "\n",
    "\n",
    "result = model1.predict(Xtest)\n",
    "print(result)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(result, Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying kmeans to the test dataset and predicting names for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Denver' 'Matthau' 'Bebe Daniels' 'Celie Johnson' 'Ben Gazzara' 'Jimmy'\n",
      " 'Matthau' 'Walter Matthau' 'Jimmy' 'Matthau' 'Tsui Hark' 'Matthau']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred = model2.predict(Xtest)\n",
    "#print (knn.kneighbors(Xtest)[1])\n",
    "print(ypred)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(ypred, Ytest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
